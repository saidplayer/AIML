{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dca28bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Feature1  Feature2 Category  Target\n",
      "0  117.640523        32        A       1\n",
      "1  104.001572        70        B       1\n",
      "2  109.787380        85        C       0\n",
      "3  122.408932        31        D       1\n",
      "4  118.675580        13        A       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a dummy dataset\n",
    "np.random.seed(0)\n",
    "dummy_data = {\n",
    "    'Feature1': np.random.normal(100, 10, 99).tolist() + [400, np.nan, np.nan],  # Normally distributed with an outlier\n",
    "    'Feature2': np.random.randint(0, 100, 102).tolist(),  # Random integers\n",
    "    'Category': ['A', 'B', 'C', 'D'] * 25 + [np.nan, 'A'],  # Categorical with some missing values\n",
    "    'Target': np.random.choice([0, 1], 102).tolist()  # Binary target variable\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df_dummy = pd.DataFrame(dummy_data)\n",
    "\n",
    "# Display the first few rows of the dummy dataset\n",
    "print(df_dummy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34d91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    return df.fillna(df.mean())  # For numeric data, fill missing values with the mean\n",
    "\n",
    "def remove_outliers(df, column_name):\n",
    "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "    return df[(z_scores < 3).all(axis=1)]  # Remove rows with any outliers\n",
    "    # Q1 = df[column_name].quantile(0.25)\n",
    "    # Q3 = df[column_name].quantile(0.75)\n",
    "    # IQR = Q3 - Q1\n",
    "    # df = df[~((df[column_name] < (Q1 - 1.5 * IQR)) | (df[column_name] > (Q3 + 1.5 * IQR)))]\n",
    "    # return df\n",
    "\n",
    "def scale_data(df, column_name):\n",
    "    scaler = StandardScaler()\n",
    "    df[df.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(df.select_dtypes(include=[np.number]))\n",
    "    # m = df[column_name].mean()\n",
    "    # s = df[column_name].std()\n",
    "    # df[column_name] = (df[column_name] - m) / s    \n",
    "    return df\n",
    "\n",
    "def encode_categorical(df, categorical_columns):\n",
    "    return pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "def save_data(df, output_filepath):\n",
    "    df.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3db581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2    Target  Category_A  Category_B  Category_C  \\\n",
      "0  1.696404 -0.513627  0.951662        True       False       False   \n",
      "1  0.336810  0.888436  0.951662       False        True       False   \n",
      "2  0.913566  1.441882 -1.050793       False       False        True   \n",
      "3  2.171740 -0.550524  0.951662       False       False       False   \n",
      "4  1.799583 -1.214659 -1.050793        True       False       False   \n",
      "5 -1.036280  0.925333  0.951662       False        True       False   \n",
      "6  0.885007  0.371886  0.951662       False       False        True   \n",
      "\n",
      "   Category_D  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3        True  \n",
      "4       False  \n",
      "5       False  \n",
      "6       False  \n",
      "     Feature1  Feature2    Target  Category_A  Category_B  Category_C  \\\n",
      "94   0.293158  0.076715  0.951662       False       False        True   \n",
      "95   0.642260 -0.771902 -1.050793       False       False       False   \n",
      "96  -0.051618  0.777747 -1.050793        True       False       False   \n",
      "97   1.718153 -0.402938  0.951662       False        True       False   \n",
      "98   0.064427 -0.587420  0.951662       False       False        True   \n",
      "100  0.292581 -0.476731  0.951662       False       False       False   \n",
      "101  0.292581 -1.030177 -1.050793        True       False       False   \n",
      "\n",
      "     Category_D  \n",
      "94        False  \n",
      "95         True  \n",
      "96        False  \n",
      "97        False  \n",
      "98        False  \n",
      "100       False  \n",
      "101       False  \n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df_preprocessed = load_data(df_dummy)\n",
    "\n",
    "# Encode categorical variables\n",
    "df_preprocessed = encode_categorical(df_preprocessed, ['Category'])\n",
    "\n",
    "# Handle missing values\n",
    "df_preprocessed = handle_missing_values(df_preprocessed)\n",
    "\n",
    "# Remove outliers\n",
    "df_preprocessed = remove_outliers(df_preprocessed, \"Feature1\")\n",
    "\n",
    "# Scale the data\n",
    "df_preprocessed = scale_data(df_preprocessed, \"Feature1\")\n",
    "# df_preprocessed = scale_data(df_preprocessed, \"Feature2\")\n",
    "\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(df_preprocessed.head(7))\n",
    "print(df_preprocessed.tail(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23a3aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Preprocessed data saved as preprocessed_dummy_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned and preprocessed DataFrame to a CSV file\n",
    "save_data(df_preprocessed, 'preprocessed_dummy_data.csv')\n",
    "\n",
    "print('Preprocessing complete. Preprocessed data saved as preprocessed_dummy_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9b817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature1      0\n",
      "Feature2      0\n",
      "Target        0\n",
      "Category_A    0\n",
      "Category_B    0\n",
      "Category_C    0\n",
      "Category_D    0\n",
      "dtype: int64\n",
      "           Feature1      Feature2        Target\n",
      "count  1.010000e+02  1.010000e+02  1.010000e+02\n",
      "mean   1.154192e-16 -2.198461e-18 -5.496154e-17\n",
      "std    1.004988e+00  1.004988e+00  1.004988e+00\n",
      "min   -2.607022e+00 -1.694312e+00 -1.050793e+00\n",
      "25%   -6.944060e-01 -6.981091e-01 -1.050793e+00\n",
      "50%    5.920661e-02 -1.815595e-01  9.516619e-01\n",
      "75%    6.647063e-01  8.515397e-01  9.516619e-01\n",
      "max    2.200511e+00  1.884639e+00  9.516619e-01\n",
      "   Feature1  Feature2    Target  Category_A  Category_B  Category_C  \\\n",
      "0  1.696404 -0.513627  0.951662        True       False       False   \n",
      "1  0.336810  0.888436  0.951662       False        True       False   \n",
      "2  0.913566  1.441882 -1.050793       False       False        True   \n",
      "3  2.171740 -0.550524  0.951662       False       False       False   \n",
      "4  1.799583 -1.214659 -1.050793        True       False       False   \n",
      "\n",
      "   Category_D  \n",
      "0       False  \n",
      "1       False  \n",
      "2       False  \n",
      "3        True  \n",
      "4       False  \n",
      "Index(['Feature1', 'Feature2', 'Target', 'Category_A', 'Category_B',\n",
      "       'Category_C', 'Category_D'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_preprocessed.isnull().sum())\n",
    "\n",
    "print(df_preprocessed.describe())\n",
    "\n",
    "print(df_preprocessed.head())\n",
    "\n",
    "print(df_preprocessed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcffa7e",
   "metadata": {},
   "source": [
    "### next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6333d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d7d57b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tenure  MonthlyCharges  TotalCharges  Churn\n",
      "0       5            70.0         350.0      1\n",
      "1      10            85.5         850.5      0\n",
      "2       3            55.3         165.9      1\n",
      "3       8            90.0         720.0      0\n",
      "4       2            65.2         130.4      1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Tenure          5 non-null      int64  \n",
      " 1   MonthlyCharges  5 non-null      float64\n",
      " 2   TotalCharges    5 non-null      float64\n",
      " 3   Churn           5 non-null      int64  \n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 292.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.drop(axis=1, columns=[\"Contract\", \"PaymentMethod\",\"CustomerID\"], inplace=True)\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "data = data.dropna()  # Simple example of dropping missing values\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=int.from_bytes(os.urandom(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "06f41e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChurnModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = nn.functional.dropout(x, 0.5, training=self.training)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = ChurnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a812c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (simplified example)\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.tensor(X_train.values).float())\n",
    "    loss = criterion(outputs.squeeze(), torch.tensor(y_train.values).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301b890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = model(torch.tensor(X_test.values).float())\n",
    "predictions = (outputs.squeeze().detach().numpy() > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test.values)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf083821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "torch.save(model.state_dict(), 'churn_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfaa43",
   "metadata": {},
   "source": [
    "### Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee2e7b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:800\n",
      " * Running on http://192.168.1.21:800\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [06/Jun/2025 19:37:09] \"GET /predict?data=1&beta=2 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data : 1\n",
      "beta : 2\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, Response, flash, redirect\n",
    "import joblib, json\n",
    "\n",
    "app = Flask(__name__)\n",
    "# model = joblib.load('model.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['GET'])\n",
    "def predict():\n",
    "    for key in request.args.keys():\n",
    "        print(key + \" : \" + request.args[key])\n",
    "    return \"<html><title>aaa</title><input>: data.text()}</input></html>\"\n",
    "    # return \"\", 201\n",
    "    # return redirect(url_for('http://google.com'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f926390",
   "metadata": {},
   "source": [
    "### deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884b6094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(model, 'iris_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3685d76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:80\n",
      " * Running on http://192.168.1.21:80\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Jun/2025 13:11:03] \"GET /predict?data=[[1,2,3,4],[4,3,2,1]] HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load('iris_model.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['GET'])\n",
    "def predict():\n",
    "    data = request.args[\"data\"]\n",
    "    data = np.array(json.loads(data)).reshape(-1,4)\n",
    "    prediction = model.predict(data)\n",
    "    return \"<html><title>aaa</title><h3>Model outcome: <i>\" + str(prediction) + \"</i></h3></html>\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1a5f0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.9'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
